# ğŸ“ Sentiment Analysis on Amazon Product Reviews

## ğŸ“Œ Project Summary

This project builds a **multi-class sentiment classifier** to categorize Amazon-style product reviews as **positive**, **neutral**, or **negative**. Mock data is generated using the `Faker` library, and multiple models are trained to evaluate classification performance.

---

## ğŸ¯ Objectives

* Generate synthetic review data
* Preprocess text (cleaning, tokenization, stopword removal)
* Train sentiment classifiers: Naive Bayes and LSTM
* Evaluate performance with classification reports and confusion matrix
* Visualize sentiment patterns using word clouds

---

## ğŸ›  Key Components

### 1. **Data Generation**

* 5,000 synthetic reviews generated using `Faker`
* Balanced distribution across 3 sentiment labels

### 2. **Preprocessing**

* Cleaned text using regex, tokenization, and stopword removal (NLTK)
* Split into train/test sets (80/20 stratified)

### 3. **Modeling**

#### ğŸ§ª Naive Bayes (with TF-IDF)

* GridSearchCV used for hyperparameter tuning
* TF-IDF features extracted with n-grams and stopword filtering

#### ğŸ§  LSTM Model (Keras)

* Tokenized and padded sequences
* Simple LSTM architecture: Embedding â†’ LSTM â†’ Dropout â†’ Dense layers
* Trained with `sparse_categorical_crossentropy` loss

---

## ğŸ“Š Evaluation

* **Naive Bayes**

  * Accuracy â‰ˆ 34%
  * Good recall for neutral class; poor for positive/negative

* **LSTM**

  * Accuracy stagnated (\~35%); likely due to data simplicity

* **Confusion Matrix**: Visualized misclassifications

* **Classification Report**: Showed precision, recall, F1-score

* **Word Clouds**: Highlighted frequent terms by sentiment

---

## ğŸ“¦ Dependencies

* `pandas`, `numpy`, `nltk`
* `scikit-learn`
* `faker`, `matplotlib`, `seaborn`
* `keras`, `tensorflow`
* `wordcloud`

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ sentiment_analysis_notebook.ipynb
â”œâ”€â”€ Task2_README.md
â”œâ”€â”€ data/ (optional, for real datasets)
â””â”€â”€ models/ (optional, for saved models)
```

---

## ğŸš§ Limitations & Notes

* LSTM and Naive Bayes underperform due to **synthetic randomness** of fake reviews
* For real-world accuracy, use authentic labeled reviews (e.g., from Amazon)